{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href=\"#Problem-research\" data-toc-modified-id=\"Problem-research-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Problem research</a></span></li><li><span><a href=\"#Dealing-with-imbalance\" data-toc-modified-id=\"Dealing-with-imbalance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dealing with imbalance</a></span></li><li><span><a href=\"#Model-testing\" data-toc-modified-id=\"Model-testing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model testing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers began to leave Beta-Bank. Every month. A little, but noticeable. Banking marketers figured it was cheaper to keep current customers than to attract new ones.\n",
    "\n",
    "It is necessary to predict whether the client will leave the bank in the near future or not. Provided historical data on customer behavior and termination of agreements with the bank.\n",
    "\n",
    "It is necessary to build a model with an extremely large *F1*-measure (above 0.59).\n",
    "\n",
    "Additionally, it is necessary to measure *AUC-ROC* and compare its value with *F1*-measure.\n",
    "\n",
    "Data source: [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "#bd = pd.read_csv('Churn.csv') #bd - tariff_data\n",
    "bd = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9091 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4.0</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0           1    15634602   Hargrave          619    France  Female   42   \n",
       "1           2    15647311       Hill          608     Spain  Female   41   \n",
       "2           3    15619304       Onio          502    France  Female   42   \n",
       "3           4    15701354       Boni          699    France  Female   39   \n",
       "4           5    15737888   Mitchell          850     Spain  Female   43   \n",
       "5           6    15574012        Chu          645     Spain    Male   44   \n",
       "6           7    15592531   Bartlett          822    France    Male   50   \n",
       "7           8    15656148     Obinna          376   Germany  Female   29   \n",
       "8           9    15792365         He          501    France    Male   44   \n",
       "9          10    15592389         H?          684    France    Male   27   \n",
       "10         11    15767821     Bearce          528    France    Male   31   \n",
       "11         12    15737173    Andrews          497     Spain    Male   24   \n",
       "12         13    15632264        Kay          476    France  Female   34   \n",
       "13         14    15691483       Chin          549    France  Female   25   \n",
       "14         15    15600882      Scott          635     Spain  Female   35   \n",
       "15         16    15643966    Goforth          616   Germany    Male   45   \n",
       "16         17    15737452      Romeo          653   Germany    Male   58   \n",
       "17         18    15788218  Henderson          549     Spain  Female   24   \n",
       "18         19    15661507    Muldrow          587     Spain    Male   45   \n",
       "19         20    15568982        Hao          726    France  Female   24   \n",
       "\n",
       "    Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0      2.0       0.00              1          1               1   \n",
       "1      1.0   83807.86              1          0               1   \n",
       "2      8.0  159660.80              3          1               0   \n",
       "3      1.0       0.00              2          0               0   \n",
       "4      2.0  125510.82              1          1               1   \n",
       "5      8.0  113755.78              2          1               0   \n",
       "6      7.0       0.00              2          1               1   \n",
       "7      4.0  115046.74              4          1               0   \n",
       "8      4.0  142051.07              2          0               1   \n",
       "9      2.0  134603.88              1          1               1   \n",
       "10     6.0  102016.72              2          0               0   \n",
       "11     3.0       0.00              2          1               0   \n",
       "12    10.0       0.00              2          1               0   \n",
       "13     5.0       0.00              2          0               0   \n",
       "14     7.0       0.00              2          1               1   \n",
       "15     3.0  143129.41              2          0               1   \n",
       "16     1.0  132602.88              1          1               0   \n",
       "17     9.0       0.00              2          1               1   \n",
       "18     6.0       0.00              1          0               0   \n",
       "19     6.0       0.00              2          1               1   \n",
       "\n",
       "    EstimatedSalary  Exited  \n",
       "0         101348.88       1  \n",
       "1         112542.58       0  \n",
       "2         113931.57       1  \n",
       "3          93826.63       0  \n",
       "4          79084.10       0  \n",
       "5         149756.71       1  \n",
       "6          10062.80       0  \n",
       "7         119346.88       1  \n",
       "8          74940.50       0  \n",
       "9          71725.73       0  \n",
       "10         80181.12       0  \n",
       "11         76390.01       0  \n",
       "12         26260.98       0  \n",
       "13        190857.79       0  \n",
       "14         65951.65       0  \n",
       "15         64327.26       0  \n",
       "16          5097.67       1  \n",
       "17         14406.41       0  \n",
       "18        158684.81       0  \n",
       "19         54724.03       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd.info()\n",
    "bd.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a preliminary analysis of the available data, there were gaps in the Tenure column that need to be filled in, as their share is about 9% - a significant number.\n",
    "\n",
    "Let's move on to data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = bd.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the RowNumber, CustomerId and Surname columns were removed, which do not carry any meaningful information, and the RowNumber column actually duplicates the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_dict = (\n",
    "    bd\n",
    "    .pivot_table(index=['Geography', 'Gender', 'Age','NumOfProducts'], \n",
    "                 values='Tenure', \n",
    "                 aggfunc='median')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenure_index(row):\n",
    "    Geography = row ['Geography']\n",
    "    Gender = row['Gender']\n",
    "    Age = row['Age']\n",
    "    NumOfProducts = row['NumOfProducts']\n",
    "    tenure_index = tuple([Geography, Gender, Age, NumOfProducts])\n",
    "    return tenure_index\n",
    "bd['tenure_index'] = bd.apply(tenure_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9975 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "tenure_index       10000 non-null object\n",
      "dtypes: float64(3), int64(6), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "bd['Tenure'] = (\n",
    "    bd['Tenure']\n",
    "    .fillna(bd['tenure_index'].map(tenure_dict['Tenure']))\n",
    ")\n",
    "bd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenure categorized by Geography, Gender, Age, and NumOfProducts to fill in the missing values found, calculating the median for each of the possible options. After filling, out of the initially discovered gaps in the amount of 909 pieces, 25 remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9975 entries, 0 to 9974\n",
      "Data columns (total 12 columns):\n",
      "CreditScore        9975 non-null int64\n",
      "Geography          9975 non-null object\n",
      "Gender             9975 non-null object\n",
      "Age                9975 non-null int64\n",
      "Tenure             9975 non-null float64\n",
      "Balance            9975 non-null float64\n",
      "NumOfProducts      9975 non-null int64\n",
      "HasCrCard          9975 non-null int64\n",
      "IsActiveMember     9975 non-null int64\n",
      "EstimatedSalary    9975 non-null float64\n",
      "Exited             9975 non-null int64\n",
      "tenure_index       9975 non-null object\n",
      "dtypes: float64(3), int64(6), object(3)\n",
      "memory usage: 935.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bd.dropna(subset = ['Tenure'], inplace = True)\n",
    "bd.reset_index(drop=True, inplace=True)\n",
    "bd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9975 entries, 0 to 9974\n",
      "Data columns (total 11 columns):\n",
      "CreditScore        9975 non-null int64\n",
      "Geography          9975 non-null object\n",
      "Gender             9975 non-null object\n",
      "Age                9975 non-null int64\n",
      "Tenure             9975 non-null float64\n",
      "Balance            9975 non-null float64\n",
      "NumOfProducts      9975 non-null int64\n",
      "HasCrCard          9975 non-null int64\n",
      "IsActiveMember     9975 non-null int64\n",
      "EstimatedSalary    9975 non-null float64\n",
      "Exited             9975 non-null int64\n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 857.4+ KB\n"
     ]
    }
   ],
   "source": [
    "bd = bd.drop(['tenure_index'], axis = 1)\n",
    "bd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid duplication when applying categorization with fewer parameters, rows with gaps have been removed along with the no longer needed categorization index column.\n",
    "\n",
    "Let's move on to feature transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9975 entries, 0 to 9974\n",
      "Data columns (total 12 columns):\n",
      "CreditScore          9975 non-null int64\n",
      "Age                  9975 non-null int64\n",
      "Tenure               9975 non-null float64\n",
      "Balance              9975 non-null float64\n",
      "NumOfProducts        9975 non-null int64\n",
      "HasCrCard            9975 non-null int64\n",
      "IsActiveMember       9975 non-null int64\n",
      "EstimatedSalary      9975 non-null float64\n",
      "Exited               9975 non-null int64\n",
      "Geography_Germany    9975 non-null uint8\n",
      "Geography_Spain      9975 non-null uint8\n",
      "Gender_Male          9975 non-null uint8\n",
      "dtypes: float64(3), int64(6), uint8(3)\n",
      "memory usage: 730.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>645</td>\n",
       "      <td>44</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>822</td>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>376</td>\n",
       "      <td>29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>4.0</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>684</td>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>528</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>497</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>476</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>549</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>635</td>\n",
       "      <td>35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>616</td>\n",
       "      <td>45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>653</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>549</td>\n",
       "      <td>24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>587</td>\n",
       "      <td>45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>726</td>\n",
       "      <td>24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0           619   42     2.0       0.00              1          1   \n",
       "1           608   41     1.0   83807.86              1          0   \n",
       "2           502   42     8.0  159660.80              3          1   \n",
       "3           699   39     1.0       0.00              2          0   \n",
       "4           850   43     2.0  125510.82              1          1   \n",
       "5           645   44     8.0  113755.78              2          1   \n",
       "6           822   50     7.0       0.00              2          1   \n",
       "7           376   29     4.0  115046.74              4          1   \n",
       "8           501   44     4.0  142051.07              2          0   \n",
       "9           684   27     2.0  134603.88              1          1   \n",
       "10          528   31     6.0  102016.72              2          0   \n",
       "11          497   24     3.0       0.00              2          1   \n",
       "12          476   34    10.0       0.00              2          1   \n",
       "13          549   25     5.0       0.00              2          0   \n",
       "14          635   35     7.0       0.00              2          1   \n",
       "15          616   45     3.0  143129.41              2          0   \n",
       "16          653   58     1.0  132602.88              1          1   \n",
       "17          549   24     9.0       0.00              2          1   \n",
       "18          587   45     6.0       0.00              1          0   \n",
       "19          726   24     6.0       0.00              2          1   \n",
       "\n",
       "    IsActiveMember  EstimatedSalary  Exited  Geography_Germany  \\\n",
       "0                1        101348.88       1                  0   \n",
       "1                1        112542.58       0                  0   \n",
       "2                0        113931.57       1                  0   \n",
       "3                0         93826.63       0                  0   \n",
       "4                1         79084.10       0                  0   \n",
       "5                0        149756.71       1                  0   \n",
       "6                1         10062.80       0                  0   \n",
       "7                0        119346.88       1                  1   \n",
       "8                1         74940.50       0                  0   \n",
       "9                1         71725.73       0                  0   \n",
       "10               0         80181.12       0                  0   \n",
       "11               0         76390.01       0                  0   \n",
       "12               0         26260.98       0                  0   \n",
       "13               0        190857.79       0                  0   \n",
       "14               1         65951.65       0                  0   \n",
       "15               1         64327.26       0                  1   \n",
       "16               0          5097.67       1                  1   \n",
       "17               1         14406.41       0                  0   \n",
       "18               0        158684.81       0                  0   \n",
       "19               1         54724.03       0                  0   \n",
       "\n",
       "    Geography_Spain  Gender_Male  \n",
       "0                 0            0  \n",
       "1                 1            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 1            0  \n",
       "5                 1            1  \n",
       "6                 0            1  \n",
       "7                 0            0  \n",
       "8                 0            1  \n",
       "9                 0            1  \n",
       "10                0            1  \n",
       "11                1            1  \n",
       "12                0            0  \n",
       "13                0            0  \n",
       "14                1            0  \n",
       "15                0            1  \n",
       "16                0            1  \n",
       "17                1            0  \n",
       "18                1            1  \n",
       "19                0            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_ohe = pd.get_dummies(bd, drop_first=True)\n",
    "bd_ohe.info()\n",
    "bd_ohe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the categorical features in the data into numerical ones, a direct coding technique was used using an argument to exclude falling into a dummy trap.\n",
    "\n",
    "Let's move on to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_train, bd_valid_test = train_test_split(\n",
    "    bd_ohe, \n",
    "    test_size=0.40, \n",
    "    random_state=12345, \n",
    "    stratify=bd_ohe['Exited']\n",
    ")\n",
    "bd_valid, bd_test = train_test_split(\n",
    "    bd_valid_test,                                      \n",
    "    test_size=0.50, \n",
    "    random_state=12345, \n",
    "    stratify=bd_valid_test['Exited']\n",
    ")\n",
    "features_train = bd_train.drop(['Exited'], axis=1)\n",
    "target_train = bd_train['Exited']\n",
    "features_valid = bd_valid.drop(['Exited'], axis=1)\n",
    "target_valid = bd_valid['Exited']\n",
    "features_test = bd_test.drop(['Exited'], axis=1)\n",
    "target_test = bd_test['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data into three samples (training, validation and test), the train_test_split method was applied in two stages.  At the first stage, the first part related to the training set was selected, at the second stage, the second part was divided into half to obtain a validation and test set. The overall ratio of all three samples was 3:1:1 or 60%:20%:20%.\n",
    "Further, each of the samples was divided into features (features) and target feature. Based on the task set before us, the target feature is the fact that the client leaves. It is the 'Exited' column in the available data.\n",
    "\n",
    "Let's move on to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data standardization was applied to scale the available numerical features in the columns CreditScore, Age, Tenure, Balance, NumOfProducts, and EstimatedSalary.\n",
    "\n",
    "We turn to the study of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели решающего дерева на валидационной выборке: 0.5525525525525525\n",
      "criterion наилучшей модели решающего дерева: entropy\n",
      "max_depth наилучшей модели решающего дерева: 6\n",
      "AUC-ROC наилучшей модели решающего дерева на валидационной выборке: 0.8177172140694153\n",
      "CPU times: user 6.38 s, sys: 35.4 ms, total: 6.41 s\n",
      "Wall time: 6.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "best_model_DTC = None\n",
    "best_result_DTC = 0\n",
    "best_crit_DTC = ''\n",
    "best_depth_DTC = 0\n",
    "for crit in ['gini', 'entropy']:\n",
    "    for depth in range(1, 100):\n",
    "        model_DTC = DecisionTreeClassifier(random_state=12345, criterion=crit, max_depth=depth)\n",
    "        model_DTC.fit(features_train, target_train)\n",
    "        predicted_valid = model_DTC.predict(features_valid)\n",
    "        result_DTC = f1_score(target_valid, predicted_valid)\n",
    "        probabilities_valid = model_DTC.predict_proba(features_valid)\n",
    "        probabilities_one_valid = probabilities_valid[:, 1]\n",
    "        auc_roc_DTC = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "        if result_DTC > best_result_DTC:\n",
    "            best_model_DTC = model_DTC\n",
    "            best_result_DTC = result_DTC\n",
    "            best_crit_DTC = crit\n",
    "            best_depth_DTC = depth\n",
    "            best_auc_roc_DTC = auc_roc_DTC\n",
    "\n",
    "print(\"F1-мера наилучшей модели решающего дерева на валидационной выборке:\", best_result_DTC)\n",
    "print(\"criterion наилучшей модели решающего дерева:\", best_crit_DTC)\n",
    "print(\"max_depth наилучшей модели решающего дерева:\", best_depth_DTC)\n",
    "print(\"AUC-ROC наилучшей модели решающего дерева на валидационной выборке:\", best_auc_roc_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the choice for the model consists of two choices in the Exited column, the target feature is categorical with a binary classification.  For modeling in the case of binary classification, let's start with a decision tree model.  To improve the model, we will form a cycle with the choice of hyperparameters: criterion with the options 'gini' and 'entropy', and max_depth in the range from 1 to 100. Based on the results of the work, the model with hyperparameters criterion - entropy and max_depth - 6 turned out to be the best model in terms of the F1-measure.  The AUC-ROC for this model is round 0.82, which is better than that of the random model (0.5), but still far from 1. The F1-measure turned out to be below our threshold of 0.59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели случайного леса на валидационной выборке: 0.5611940298507462\n",
      "n_estimators наилучшей модели случайного леса: 13\n",
      "criterion наилучшей модели случайного леса: entropy\n",
      "max_depth наилучшей модели случайного леса: 19\n",
      "AUC-ROC наилучшей модели случайного леса на валидационной выборке: 0.8149110955819551\n",
      "CPU times: user 6min 13s, sys: 3.35 s, total: 6min 17s\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_RFC = None\n",
    "best_result_RFC = 0\n",
    "best_est_RFC = 0\n",
    "best_crit_RFC = ''\n",
    "best_depth_RFC = 0\n",
    "for est in range(1, 50):\n",
    "    for crit in ['gini', 'entropy']:\n",
    "        for depth in range(1, 25):\n",
    "                model_RFC = RandomForestClassifier(random_state=12345, n_estimators=est, criterion=crit, max_depth=depth)\n",
    "                model_RFC.fit(features_train, target_train)\n",
    "                predicted_valid = model_RFC.predict(features_valid)\n",
    "                result_RFC = f1_score(target_valid, predicted_valid)\n",
    "                probabilities_valid = model_RFC.predict_proba(features_valid)\n",
    "                probabilities_one_valid = probabilities_valid[:, 1]\n",
    "                auc_roc_RFC = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "                if result_RFC > best_result_RFC:\n",
    "                    best_model_RFC = model_RFC\n",
    "                    best_result_RFC = result_RFC\n",
    "                    best_est_RFC = est\n",
    "                    best_crit_RFC = crit\n",
    "                    best_depth_RFC = depth\n",
    "                    best_auc_roc_RFC = auc_roc_RFC\n",
    "\n",
    "print(\"F1-мера наилучшей модели случайного леса на валидационной выборке:\", best_result_RFC)\n",
    "print(\"n_estimators наилучшей модели случайного леса:\", best_est_RFC)\n",
    "print(\"criterion наилучшей модели случайного леса:\", best_crit_RFC)\n",
    "print(\"max_depth наилучшей модели случайного леса:\", best_depth_RFC)\n",
    "print(\"AUC-ROC наилучшей модели случайного леса на валидационной выборке:\", best_auc_roc_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second binary classification model used the random forest model. To improve the model, a cycle was formed with the choice of hyperparameters: n_estimators in the range from 1 to 50, criterion with options 'gini' and 'entropy', and max_depth in the range from 1 to 25. According to the results of the work, the model with hyperparameters n_estimators turned out to be the best -  13, criterion - entropy and max_depth - 19. The F1-measure for it was greater than that of the best decision tree model, but still less than the threshold set in front of us at 0.59.  The AUC-ROC for this model is round 0.82, the same as for the decision tree model, which is better than the random model (0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели логистической регрессии на валидационной выборке: 0.29411764705882354\n",
      "solver наилучшей модели логистической регрессии: liblinear\n",
      "AUC-ROC наилучшей модели логистической регрессии на валидационной выборке: 0.7559251494681264\n",
      "CPU times: user 378 ms, sys: 73.6 ms, total: 451 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_LR = None\n",
    "best_result_LR = 0\n",
    "best_solver_LR = ''\n",
    "for sol in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "    model_LR = LogisticRegression(random_state=12345, solver=sol)\n",
    "    model_LR.fit(features_train, target_train)\n",
    "    predicted_valid = model_LR.predict(features_valid)\n",
    "    result_LR = f1_score(target_valid, predicted_valid)\n",
    "    probabilities_valid = model_LR.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    auc_roc_LR = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "    if result_LR > best_result_LR:\n",
    "        best_model_LR = model_LR\n",
    "        best_result_LR = result_LR\n",
    "        best_solver_LR = sol\n",
    "        best_auc_roc_LR = auc_roc_LR\n",
    "\n",
    "print(\"F1-мера наилучшей модели логистической регрессии на валидационной выборке:\", best_result_LR)\n",
    "print(\"solver наилучшей модели логистической регрессии:\", best_solver_LR)\n",
    "print(\"AUC-ROC наилучшей модели логистической регрессии на валидационной выборке:\", best_auc_roc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third binary classification model used a logistic regression model. To improve the model, a loop was formed with the choice of the solver hyperparameter with options 'newton-cg', 'lbfgs', 'liblinear', 'sag' and 'saga'.  According to the results of the work carried out, the model with the hyperparameter solver - liblinear turned out to be the best. The F1-measure for it was significantly less than the threshold of 0.59 set before us, and, accordingly, less than that of the best decision tree and random forest models. The AUC-ROC of this model is also less than that of the best decision tree and random forest models, but still higher than that of the random model.\n",
    "\n",
    "We turn to the elimination of imbalances to improve the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер таблицы отрицательных признаков: (4770, 11)\n",
      "Размер таблицы положительных признаков: (1215, 11)\n",
      "Размер таблицы отрицательных значений целевого признака: (4770,)\n",
      "Размер таблицы положительных значений целевого признака: (1215,)\n"
     ]
    }
   ],
   "source": [
    "features_zeros = features_train[target_train == 0]\n",
    "features_ones = features_train[target_train == 1]\n",
    "target_zeros = target_train[target_train == 0]\n",
    "target_ones = target_train[target_train == 1]\n",
    "\n",
    "print(\"Размер таблицы отрицательных признаков:\", features_zeros.shape)\n",
    "print(\"Размер таблицы положительных признаков:\", features_ones.shape)\n",
    "print(\"Размер таблицы отрицательных значений целевого признака:\", target_zeros.shape)\n",
    "print(\"Размер таблицы положительных значений целевого признака:\", target_ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant difference in the number of objects of positive and negative class. There are many more negative signs.\n",
    "\n",
    "Let's apply two options for dealing with the detected imbalance, using the functions of random decrease and increase in samples RandomUnderSampler and RandomOverSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "features_train_over, target_train_over = oversample.fit_resample(features_train, \n",
    "                                                                 target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер таблицы отрицательных признаков после случайного увеличения: (4770, 11)\n",
      "Размер таблицы положительных признаков после случайного увеличения: (4770, 11)\n",
      "Размер таблицы отрицательных значений целевого признака после случайного увеличения: (4770,)\n",
      "Размер таблицы положительных значений целевого признака после случайного увеличения: (4770,)\n"
     ]
    }
   ],
   "source": [
    "features_zeros_over = features_train_over[target_train_over == 0]\n",
    "features_ones_over = features_train_over[target_train_over == 1]\n",
    "target_zeros_over = target_train_over[target_train_over == 0]\n",
    "target_ones_over = target_train_over[target_train_over == 1]\n",
    "\n",
    "print(\"Размер таблицы отрицательных признаков после случайного увеличения:\", \n",
    "      features_zeros_over.shape)\n",
    "print(\"Размер таблицы положительных признаков после случайного увеличения:\", \n",
    "      features_ones_over.shape)\n",
    "print(\"Размер таблицы отрицательных значений целевого признака после случайного увеличения:\", \n",
    "      target_zeros_over.shape)\n",
    "print(\"Размер таблицы положительных значений целевого признака после случайного увеличения:\", \n",
    "      target_ones_over.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "features_train_under, target_train_under = undersample.fit_resample(features_train, \n",
    "                                                                 target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер таблицы отрицательных признаков после случайного уменьшения: (1215, 11)\n",
      "Размер таблицы положительных признаков после случайного уменьшения: (1215, 11)\n",
      "Размер таблицы отрицательных значений целевого признака после случайного уменьшения: (1215,)\n",
      "Размер таблицы положительных значений целевого признака после случайного уменьшения: (1215,)\n"
     ]
    }
   ],
   "source": [
    "features_zeros_under = features_train_under[target_train_under == 0]\n",
    "features_ones_under = features_train_under[target_train_under == 1]\n",
    "target_zeros_under = target_train_under[target_train_under == 0]\n",
    "target_ones_under = target_train_under[target_train_under == 1]\n",
    "\n",
    "print(\"Размер таблицы отрицательных признаков после случайного уменьшения:\", \n",
    "      features_zeros_under.shape)\n",
    "print(\"Размер таблицы положительных признаков после случайного уменьшения:\", \n",
    "      features_ones_under.shape)\n",
    "print(\"Размер таблицы отрицательных значений целевого признака после случайного уменьшения:\", \n",
    "      target_zeros_under.shape)\n",
    "print(\"Размер таблицы положительных значений целевого признака после случайного уменьшения:\", \n",
    "      target_ones_under.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, training samples were formed with two options for dealing with imbalance. Next, we will re-select the models, but using these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели решающего дерева на валидационной выборке после случайного увеличения: 0.5472727272727272\n",
      "criterion наилучшей модели решающего дерева после случайного увеличения: gini\n",
      "max_depth наилучшей модели решающего дерева после случайного увеличения: 5\n",
      "AUC-ROC наилучшей модели решающего дерева после случайного увеличения на валидационной выборке: 0.8150415404922743\n",
      "CPU times: user 7.87 s, sys: 64.8 ms, total: 7.93 s\n",
      "Wall time: 8.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "best_model_DTC_over = None\n",
    "best_result_DTC_over = 0\n",
    "best_crit_DTC_over = ''\n",
    "best_depth_DTC_over = 0\n",
    "best_auc_roc_DTC_over = 0\n",
    "for crit in ['gini', 'entropy']:\n",
    "    for depth in range(1, 100):\n",
    "        model_DTC_over = DecisionTreeClassifier(random_state=12345, criterion=crit, max_depth=depth)\n",
    "        model_DTC_over.fit(features_train_over, target_train_over)\n",
    "        predicted_valid = model_DTC_over.predict(features_valid)\n",
    "        result_DTC_over = f1_score(target_valid, predicted_valid)\n",
    "        probabilities_valid = model_DTC_over.predict_proba(features_valid)\n",
    "        probabilities_one_valid = probabilities_valid[:, 1]\n",
    "        auc_roc_DTC_over = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "        if result_DTC_over > best_result_DTC_over:\n",
    "            best_model_DTC_over = model_DTC_over\n",
    "            best_result_DTC_over = result_DTC_over\n",
    "            best_crit_DTC_over = crit\n",
    "            best_depth_DTC_over = depth\n",
    "            best_auc_roc_DTC_over = auc_roc_DTC_over\n",
    "\n",
    "print(\"F1-мера наилучшей модели решающего дерева на валидационной выборке после случайного увеличения:\", \n",
    "      best_result_DTC_over)\n",
    "print(\"criterion наилучшей модели решающего дерева после случайного увеличения:\", best_crit_DTC_over)\n",
    "print(\"max_depth наилучшей модели решающего дерева после случайного увеличения:\", best_depth_DTC_over)\n",
    "print(\"AUC-ROC наилучшей модели решающего дерева после случайного увеличения на валидационной выборке:\", \n",
    "      best_auc_roc_DTC_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model, we will form a cycle with the choice of hyperparameters: criterion with options 'gini' and 'entropy', and max_depth in the range from 1 to 100. According to the results of the work, the best model after randomly increasing the F1-measure turned out to be a model with hyperparameters criterion - entropy and  max_depth - 5. AUC-ROC for this model increased slightly and is rounded also 0.82, which is better than that of the random model (0.5), but still far from 1. F1-measure also increased and was rounded 0.56,  which is still less than the threshold of 0.59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели случайного леса на валидационной выборке после случайного увеличения: 0.6079182630906769\n",
      "n_estimators наилучшей модели случайного леса после случайного увеличения: 43\n",
      "criterion наилучшей модели случайного леса после случайного увеличения: entropy\n",
      "max_depth наилучшей модели случайного леса после случайного увеличения: 14\n",
      "AUC-ROC наилучшей модели случайного леса после случайного увеличения на валидационной выборке: 0.8409255376970262\n",
      "CPU times: user 7min 53s, sys: 2.7 s, total: 7min 55s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_RFC_over = None\n",
    "best_result_RFC_over = 0\n",
    "best_est_RFC_over = 0\n",
    "best_crit_RFC_over = ''\n",
    "best_depth_RFC_over = 0\n",
    "best_auc_roc_RFC_over = 0\n",
    "for est in range(1, 50):\n",
    "    for crit in ['gini', 'entropy']:\n",
    "        for depth in range(1, 25):\n",
    "                model_RFC_over = RandomForestClassifier(random_state=12345, n_estimators=est, criterion=crit, max_depth=depth)\n",
    "                model_RFC_over.fit(features_train_over, target_train_over)\n",
    "                predicted_valid = model_RFC_over.predict(features_valid)\n",
    "                result_RFC_over = f1_score(target_valid, predicted_valid)\n",
    "                probabilities_valid = model_RFC_over.predict_proba(features_valid)\n",
    "                probabilities_one_valid = probabilities_valid[:, 1]\n",
    "                auc_roc_RFC_over = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "                if result_RFC_over > best_result_RFC_over:\n",
    "                    best_model_RFC_over = model_RFC_over\n",
    "                    best_result_RFC_over = result_RFC_over\n",
    "                    best_est_RFC_over = est\n",
    "                    best_crit_RFC_over = crit\n",
    "                    best_depth_RFC_over = depth\n",
    "                    best_auc_roc_RFC_over = auc_roc_RFC_over\n",
    "\n",
    "print(\"F1-мера наилучшей модели случайного леса на валидационной выборке после случайного увеличения:\", \n",
    "      best_result_RFC_over)\n",
    "print(\"n_estimators наилучшей модели случайного леса после случайного увеличения:\", best_est_RFC_over)\n",
    "print(\"criterion наилучшей модели случайного леса после случайного увеличения:\", best_crit_RFC_over)\n",
    "print(\"max_depth наилучшей модели случайного леса после случайного увеличения:\", best_depth_RFC_over)\n",
    "print(\"AUC-ROC наилучшей модели случайного леса после случайного увеличения на валидационной выборке:\", \n",
    "      best_auc_roc_RFC_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the random forest model, a cycle was formed with the choice of hyperparameters: n_estimators in the range from 1 to 50, criterion with options 'gini' and 'entropy', and max_depth in the range from 1 to 25. According to the results of the work, the best model after random increase turned out to be  the model with hyperparameters n_estimators - 16, criterion - entropy and max_depth - 11. The F1-measure for it increased significantly compared to the model without random increase and amounted to 0.62, which is more than the threshold set in front of us at 0.59. The AUC-ROC for this model is round 0.84, an improvement over the past and currently the highest of any model studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели логистической регрессии на валидационной выборке после случайного увеличения: 0.495985727029438\n",
      "solver наилучшей модели логистической регрессии после случайного увеличения: newton-cg\n",
      "AUC-ROC наилучшей модели логистической регрессии на валидационной выборке после случайного увеличения: 0.7611802158552682\n",
      "CPU times: user 512 ms, sys: 109 ms, total: 621 ms\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_LR_over = None\n",
    "best_result_LR_over = 0\n",
    "best_solver_LR_over = ''\n",
    "best_auc_roc_LR_over = 0\n",
    "for sol in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "    model_LR_over = LogisticRegression(random_state=12345, solver=sol)\n",
    "    model_LR_over.fit(features_train_over, target_train_over)\n",
    "    predicted_valid = model_LR_over.predict(features_valid)\n",
    "    result_LR_over = f1_score(target_valid, predicted_valid)\n",
    "    probabilities_valid = model_LR_over.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    auc_roc_LR_over = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "    if result_LR_over > best_result_LR_over:\n",
    "        best_model_LR_over = model_LR_over\n",
    "        best_result_LR_over = result_LR_over\n",
    "        best_solver_LR_over = sol\n",
    "        best_auc_roc_LR_over = auc_roc_LR_over\n",
    "\n",
    "print(\"F1-мера наилучшей модели логистической регрессии на валидационной выборке после случайного увеличения:\", \n",
    "      best_result_LR_over)\n",
    "print(\"solver наилучшей модели логистической регрессии после случайного увеличения:\", best_solver_LR_over)\n",
    "print(\"AUC-ROC наилучшей модели логистической регрессии на валидационной выборке после случайного увеличения:\", \n",
    "      best_auc_roc_LR_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best logistic regression model, as well as the decision tree and random forest models, showed better results after random increase than before random increase, but this did not help it rise from the last place in terms of indicators - they are still the same low and significantly worse.  from the best results of other models.\n",
    "\n",
    "Next, we present a grouped selection of decision tree, random forest, and logistic regression models on samples with random reduction, and then draw a general conclusion after dealing with the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели решающего дерева на валидационной выборке после случайного уменьшения: 0.5517948717948719\n",
      "criterion наилучшей модели решающего дерева после случайного уменьшения: gini\n",
      "max_depth наилучшей модели решающего дерева после случайного уменьшения: 6\n",
      "AUC-ROC наилучшей модели решающего дерева на валидационной выборке после случайного уменьшения: 0.8161712865905739\n",
      "CPU times: user 3.33 s, sys: 65.1 ms, total: 3.4 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "best_model_DTC_under = None\n",
    "best_result_DTC_under = 0\n",
    "best_crit_DTC_under = ''\n",
    "best_depth_DTC_under = 0\n",
    "best_auc_roc_DTC_under = 0\n",
    "for crit in ['gini', 'entropy']:\n",
    "    for depth in range(1, 100):\n",
    "        model_DTC_under = DecisionTreeClassifier(random_state=12345, criterion=crit, max_depth=depth)\n",
    "        model_DTC_under.fit(features_train_under, target_train_under)\n",
    "        predicted_valid = model_DTC_under.predict(features_valid)\n",
    "        result_DTC_under = f1_score(target_valid, predicted_valid)\n",
    "        probabilities_valid = model_DTC_under.predict_proba(features_valid)\n",
    "        probabilities_one_valid = probabilities_valid[:, 1]\n",
    "        auc_roc_DTC_under = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "        if result_DTC_under > best_result_DTC_under:\n",
    "            best_model_DTC_under = model_DTC_under\n",
    "            best_result_DTC_under = result_DTC_under\n",
    "            best_crit_DTC_under = crit\n",
    "            best_depth_DTC_under = depth\n",
    "            best_auc_roc_DTC_under = auc_roc_DTC_under\n",
    "\n",
    "print(\"F1-мера наилучшей модели решающего дерева на валидационной выборке после случайного уменьшения:\", \n",
    "      best_result_DTC_under)\n",
    "print(\"criterion наилучшей модели решающего дерева после случайного уменьшения:\", best_crit_DTC_under)\n",
    "print(\"max_depth наилучшей модели решающего дерева после случайного уменьшения:\", best_depth_DTC_under)\n",
    "print(\"AUC-ROC наилучшей модели решающего дерева на валидационной выборке после случайного уменьшения:\", \n",
    "      best_auc_roc_DTC_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели случайного леса на валидационной выборке после случайного уменьшения: 0.5816023738872403\n",
      "n_estimators наилучшей модели случайного леса после случайного уменьшения: 34\n",
      "criterion наилучшей модели случайного леса после случайного уменьшения: gini\n",
      "max_depth наилучшей модели случайного леса после случайного уменьшения: 4\n",
      "AUC-ROC наилучшей модели случайного леса на валидационной выборке после случайного уменьшения: 0.8371868933923442\n",
      "CPU times: user 3min 29s, sys: 864 ms, total: 3min 29s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_RFC_under = None\n",
    "best_result_RFC_under = 0\n",
    "best_est_RFC_under = 0\n",
    "best_crit_RFC_under = ''\n",
    "best_depth_RFC_under = 0\n",
    "best_auc_roc_RFC_under = 0\n",
    "for est in range(1, 50):\n",
    "    for crit in ['gini', 'entropy']:\n",
    "        for depth in range(1, 25):\n",
    "                model_RFC_under = RandomForestClassifier(random_state=12345, n_estimators=est, criterion=crit, max_depth=depth)\n",
    "                model_RFC_under.fit(features_train_under, target_train_under)\n",
    "                predicted_valid = model_RFC_under.predict(features_valid)\n",
    "                result_RFC_under = f1_score(target_valid, predicted_valid)\n",
    "                probabilities_valid = model_RFC_under.predict_proba(features_valid)\n",
    "                probabilities_one_valid = probabilities_valid[:, 1]\n",
    "                auc_roc_RFC_under = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "                if result_RFC_under > best_result_RFC_under:\n",
    "                    best_model_RFC_under = model_RFC_under\n",
    "                    best_result_RFC_under = result_RFC_under\n",
    "                    best_est_RFC_under = est\n",
    "                    best_crit_RFC_under = crit\n",
    "                    best_depth_RFC_under = depth\n",
    "                    best_auc_roc_RFC_under = auc_roc_RFC_under\n",
    "\n",
    "print(\"F1-мера наилучшей модели случайного леса на валидационной выборке после случайного уменьшения:\", \n",
    "      best_result_RFC_under)\n",
    "print(\"n_estimators наилучшей модели случайного леса после случайного уменьшения:\", best_est_RFC_under)\n",
    "print(\"criterion наилучшей модели случайного леса после случайного уменьшения:\", best_crit_RFC_under)\n",
    "print(\"max_depth наилучшей модели случайного леса после случайного уменьшения:\", best_depth_RFC_under)\n",
    "print(\"AUC-ROC наилучшей модели случайного леса на валидационной выборке после случайного уменьшения:\", \n",
    "      best_auc_roc_RFC_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели логистической регрессии на валидационной выборке после случайного уменьшения: 0.4968944099378882\n",
      "solver наилучшей модели логистической регрессии после случайного уменьшения: newton-cg\n",
      "AUC-ROC наилучшей модели логистической регрессии на валидационной выборке после случайного уменьшения: 0.7602329373398556\n",
      "CPU times: user 228 ms, sys: 61.5 ms, total: 289 ms\n",
      "Wall time: 85.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model_LR_under = None\n",
    "best_result_LR_under = 0\n",
    "best_solver_LR_under = ''\n",
    "best_auc_roc_LR_under = 0\n",
    "for sol in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "    model_LR_under = LogisticRegression(random_state=12345, solver=sol)\n",
    "    model_LR_under.fit(features_train_under, target_train_under)\n",
    "    predicted_valid = model_LR_under.predict(features_valid)\n",
    "    result_LR_under = f1_score(target_valid, predicted_valid)\n",
    "    probabilities_valid = model_LR_under.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    auc_roc_LR_under = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "    if result_LR_under > best_result_LR_under:\n",
    "        best_model_LR_under = model_LR_under\n",
    "        best_result_LR_under = result_LR_under\n",
    "        best_solver_LR_under = sol\n",
    "        best_auc_roc_LR_under = auc_roc_LR_under\n",
    "\n",
    "print(\"F1-мера наилучшей модели логистической регрессии на валидационной выборке после случайного уменьшения:\", \n",
    "      best_result_LR_under)\n",
    "print(\"solver наилучшей модели логистической регрессии после случайного уменьшения:\", best_solver_LR_under)\n",
    "print(\"AUC-ROC наилучшей модели логистической регрессии на валидационной выборке после случайного уменьшения:\", \n",
    "      best_auc_roc_LR_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing the prepared samples after random increase and decrease on the best decision tree, random forest and logistic regression models, mixed results were obtained.\n",
    "The main thing that can be noted is that the use of training samples after a random decrease led to a drop in some of the indicators of the models, but at the same time, the indicators of the logistic regression model in this case increased, but did not exceed the required level of the F1-measure (0.59), and F1-measure (and only it without the AUC-ROC indicator) increased and reached 0.59.\n",
    "In the case of sampling, after a random increase, the performance of all models increased, although in some places by a very small amount. The leader in all indicators, as before the application of the fight against imbalance, was the random forest model.\n",
    "Due to the results obtained after adjusting the imbalance, further testing will be performed on the best random forest model with the following parameters: n_estimators - 16, criterion - entropy, max_depth - 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_valid_over = features_train_over.append(features_valid)\n",
    "target_train_valid_over = target_train_over.append(target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели случайного леса на тестовой выборке после случайного увеличения: 0.6230769230769231\n",
      "AUC-ROC наилучшей модели случайного леса на тестовой выборке после случайного увеличения: 0.8566953956052488\n"
     ]
    }
   ],
   "source": [
    "best_model_RFC_over.fit(features_train_valid_over, target_train_valid_over)\n",
    "predicted_test_over = best_model_RFC_over.predict(features_test)\n",
    "result_test_over = f1_score(target_test, predicted_test_over)\n",
    "probabilities_test_over = best_model_RFC_over.predict_proba(features_test)\n",
    "probabilities_one_test_over = probabilities_test_over[:, 1]\n",
    "auc_roc_test_over = roc_auc_score(target_test, probabilities_one_test_over)\n",
    "print(\"F1-мера наилучшей модели случайного леса на тестовой выборке после случайного увеличения:\", \n",
    "      result_test_over)\n",
    "print(\"AUC-ROC наилучшей модели случайного леса на тестовой выборке после случайного увеличения:\", \n",
    "      auc_roc_test_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best random tree model successfully passed the final testing with the improvement of previously obtained control parameters.\n",
    "\n",
    "The results of our work have shown the importance of dealing with imbalances and a significant improvement in results after its application.\n",
    "\n",
    "We transfer this model to further work on predicting the outflow of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
